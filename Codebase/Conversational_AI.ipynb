{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI5D7BqJbfZL"
      },
      "source": [
        "# Required Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aInW4uUEkk2",
        "outputId": "38f48531-f10d-49d5-92ad-5c5abd9ae6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.24.73)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.73 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.27.73)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.73->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.73->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.73->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libenchant-dev is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip install transformers\n",
        "!pip install pyenchant\n",
        "!apt-get install -y libenchant-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "246b458aa4b24630ab57c34a64d5c637",
            "f214cff051f44a078e842e06810aac4c",
            "0daf4d93e63b42738f796d28af978802",
            "0f0d4af742c94050b2ca184dab934982",
            "26ebe65f2ee34317aa7d70e9658e4972",
            "e50072d0d16f49cdb479145e026c742a",
            "884f5293cc4741b3a20e79cb467f71db",
            "bbf7299be91045f297cdb325e9cc2b8a",
            "7b31ba3591eb429ab48d39f399f85e1b",
            "cd2b2d06d6b045f48a49c449e8ae46fa",
            "181d43730b724d4996ed2166c2c2f4b2",
            "9fcefbf2f4ef42af82190d139556f493",
            "3b4ccab2b0204d7e99c4d76744db2b54",
            "549a4f7847d94e54ac89697ec153e60f",
            "fd7ccbb1380a4769bb4c5ba48588382d",
            "d8c0d5b5af3645b885487c1dd16846f4",
            "6c2495a39c0a48ec89691a659df0aa28",
            "0bcfa8f47efb4278bb344355b6e99ab9",
            "e1c32d1cc2134baf9ceda62ee234a55b",
            "cb8aa361c379455fbedce1765ff40d57",
            "c86959439f954c21a04b24b1b16e1652",
            "8f3cb368fa8242f2a10d7055a8cc7cd7"
          ]
        },
        "id": "0i5B1U5zD_ZZ",
        "outputId": "beb4184c-7834-42ee-97c8-19aa821c368a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "246b458aa4b24630ab57c34a64d5c637",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fcefbf2f4ef42af82190d139556f493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/457M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of OpenAIGPTDoubleHeadsModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['multiple_choice_head.summary.bias', 'lm_head.weight', 'multiple_choice_head.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4f69523ccb28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIGPTDoubleHeadsModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'openai-gpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIGPTTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'openai-gpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'OpenAIGPTTokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGKpCw6fFLdp"
      },
      "outputs": [],
      "source": [
        "words = ['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.', '<speaker1>', 'hello', 'how', 'are', 'you', '?', '<speaker2>', 'i', 'am', 'fine', 'thanks', '.', '<speaker1>', 'great', 'to', 'hear', '<eos>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFFNmW65FVdp"
      },
      "outputs": [],
      "source": [
        "print(words)\n",
        "words = tokenizer.convert_tokens_to_ids(words)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVxtvWARFs4t"
      },
      "outputs": [],
      "source": [
        "words_distractor = ['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.', '<speaker1>', 'hello', 'how', 'are', 'you', '?', '<speaker2>', 'i', 'am', 'fine', 'thanks', '.', '<speaker1>', 'sorry', 'to', 'hear', 'that', '<eos>'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41puiCx8FwLj"
      },
      "outputs": [],
      "source": [
        "print(words_distractor)\n",
        "words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
        "print(words_distractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo0nxWmUGVwC"
      },
      "outputs": [],
      "source": [
        "# Store the position of the last tokens for the next-sentence prediction loss\n",
        "last_token = len(words) - 1\n",
        "last_token_distractor = len(words_distractor) - 1\n",
        "\n",
        "# Now we can pad reply and distractor inputs and targets to the same length\n",
        "padding_length = max(len(words), len(words_distractor))\n",
        "def pad(x, padding):\n",
        "    return x + [padding] * (padding_length - len(x))\n",
        "\n",
        "(words, words_distractor) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
        "                                   for x in (words, words_distractor)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e-qiZTjGdC6"
      },
      "outputs": [],
      "source": [
        "print(words,\"\\n\\n\",words_distractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGLBAxuEGwlh"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([[words, words_distractor]], dtype=torch.long)\n",
        "print(input_ids)\n",
        "print(input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3LlV97eLkM2"
      },
      "outputs": [],
      "source": [
        "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
        "tokenizer.add_special_tokens({'cls_token': '[CLS]'})  # Add a [CLS] to the vocabulary (we should train it also!)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n",
        "input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n",
        "print(\"input_ids_shape: \",input_ids.shape)\n",
        "mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1\n",
        "print(\"mc_token_ids_shape: \",mc_token_ids.shape)\n",
        "\n",
        "outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
        "lm_prediction_scores, mc_prediction_scores = outputs[:2]\n",
        "\n",
        "#print(lm_prediction_scores,\"\\n\",lm_prediction_scores.shape,\"\\n\",mc_prediction_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNB1rbzCG4TN"
      },
      "outputs": [],
      "source": [
        "def tokenize(obj):\n",
        "    if isinstance(obj, str):\n",
        "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "    if isinstance(obj, dict):\n",
        "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "    return list(tokenize(o) for o in obj)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdapRKVMbnni"
      },
      "outputs": [],
      "source": [
        "url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "\n",
        "# Download and load JSON dataset\n",
        "personachat_file = cached_path(url)\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.loads(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To0u-BUbbuxE"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train']\n",
        "print(\"size of training set: \",len(train_dataset),\"\\n\")\n",
        "\n",
        "train_dataset = train_dataset[0]\n",
        "persona = train_dataset['personality']\n",
        "print('personality\\n')\n",
        "for s in persona:\n",
        "  print(s)\n",
        "print(\"\\n\")\n",
        "\n",
        "utterances = train_dataset['utterances']\n",
        "print(\"length of utterances[number of dicts]: \",len(utterances),\"\\n\")\n",
        "for d in utterances:\n",
        "  print(\"\\n\")\n",
        "  for key in d:\n",
        "    print(key,\"\\n\")\n",
        "    for s in d[key]:\n",
        "      print(s)\n",
        "    print(\"\\n\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awtWny3Vd1QR"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"valid\"]\n",
        "print(len(train_dataset),\", \",len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64yupvggeYf9"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"] = dataset[\"train\"][:1]\n",
        "dataset[\"valid\"] = dataset[\"valid\"][:1]\n",
        "print(len(dataset[\"train\"]),\" \",len(dataset[\"valid\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1A1ScApe_uy"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"train\"])\n",
        "dataset = tokenize(dataset)\n",
        "print(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3yqxe5Oc0Bm"
      },
      "outputs": [],
      "source": [
        "personality_permutations = 1\n",
        "numOfcandidates = 2\n",
        "max_history = 2\n",
        "\n",
        "def pad_dataset(dataset, padding=0):\n",
        "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
        "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "    for name in PADDED_INPUTS:\n",
        "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "    return dataset\n",
        "\n",
        "def build_input_from_segments(persona, history, reply, lm_labels=False, with_eos=True):\n",
        "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
        "    #print(persona)\n",
        "    sequence =  [[bos] + list(chain(*persona))] + history + [reply + [eos] ]\n",
        "    # print(sequence)\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
        "    # print(\"sequence: \\n\",sequence)\n",
        "    instance = {}\n",
        "    instance[\"input_ids\"] = list(chain(*sequence))\n",
        "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "    print(\"token_type_ids:\\n\", instance['token_type_ids'])\n",
        "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
        "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
        "    if lm_labels:\n",
        "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "    return instance\n",
        "\n",
        "\n",
        "def get_data_loaders(dataset):\n",
        "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
        "    personachat = dataset\n",
        "\n",
        "    #print(\"Build inputs and labels\")\n",
        "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
        "    for dataset_name, dataset in personachat.items():\n",
        "        #print(dataset_name,\": len = \",len(dataset))\n",
        "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
        "        #print(\"num of candidates: \",num_candidates)\n",
        "        if numOfcandidates > 0 and dataset_name == 'train':\n",
        "            num_candidates = min(num_candidates, numOfcandidates)\n",
        "        for dialog in dataset:\n",
        "            persona = dialog[\"personality\"].copy()\n",
        "            for _ in range(personality_permutations):\n",
        "                for utterance in dialog[\"utterances\"]:\n",
        "                    history = utterance[\"history\"][-(2*max_history+1):]\n",
        "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "                        lm_labels = bool(j == num_candidates-1)\n",
        "                        instance = build_input_from_segments(persona, history, candidate, lm_labels)\n",
        "                        for input_name, input_array in instance.items():\n",
        "                            datasets[dataset_name][input_name].append(input_array)\n",
        "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
        "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
        "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
        "    \n",
        "    # for key, val in datasets[\"train\"].items():\n",
        "    #   print(key)\n",
        "    #   if isinstance(val, list):\n",
        "    #     print(len(val))\n",
        "    print(\"Pad inputs and convert to Tensor\")\n",
        "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "        print(dataset_name,\": \")\n",
        "        # for key, val in dataset.items():\n",
        "        #   print(key,\": \",type(val))\n",
        "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
        "        for input_name in MODEL_INPUTS:\n",
        "            # print(input_name,\": \" ,type(dataset))\n",
        "            tensor = torch.tensor(dataset[input_name])\n",
        "            print(input_name,\": \",tensor.shape)\n",
        "            if input_name != \"mc_labels\":\n",
        "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
        "                print(\"After reshaping \",input_name,\": \",tensor.shape,\"\\n\")\n",
        "            tensor_datasets[dataset_name].append(tensor)\n",
        "    return tensor_datasets\n",
        "\n",
        "    # logger.info(\"Build train and validation dataloaders\")\n",
        "    # train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
        "    # train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
        "    # valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if args.distributed else None\n",
        "    # train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, shuffle=(not args.distributed))\n",
        "    # valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.valid_batch_size, shuffle=False)\n",
        "\n",
        "    # logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
        "    # logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
        "    # return train_loader, valid_loader, train_sampler, valid_sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmCjWbz-ZllK"
      },
      "outputs": [],
      "source": [
        "tensor_dataset = get_data_loaders(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQlJ8u-iTaqx"
      },
      "outputs": [],
      "source": [
        "for dataset_name, dataset in tensor_dataset.items():\n",
        "  print(dataset_name,\":\")\n",
        "  print(len(dataset))\n",
        "  for input in dataset:\n",
        "    print(input.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_c63jGqhXgz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGW2XzD5hZ1h"
      },
      "source": [
        "# My training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "C8Rh0JZQtspC",
        "outputId": "e55c4fcd-8c5e-4b20-882f-d48f59a749f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe following will be the steps in my training and evaluation process: (the entire process has been distilled down to high-level steps.\\n                                                                        Each step will be elaborated with corresponding code\\n                                                                        )\\n1. I will load raw json data from Google drive\\n2. Apply some preprocessing to clean the dataset\\n3. Finally reshape and transform the dataset to feed it to DataLoader.\\n4. Train for a number of epochs. Evaluate after each epoch.\\n5. After training is done, I can test the model with some hand-crafted (or freshly fetched) test-data\\n6. Establish a way of talking to the model - get a continuous conversation going\\n7. Host the system on a site [not here]\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "The following will be the steps in my training and evaluation process: (the entire process has been distilled down to high-level steps.\n",
        "                                                                        Each step will be elaborated with corresponding code\n",
        "                                                                        )\n",
        "1. I will load raw json data from Google drive\n",
        "2. Apply some preprocessing to clean the dataset\n",
        "3. Finally reshape and transform the dataset to feed it to DataLoader.\n",
        "4. Train for a number of epochs. Evaluate after each epoch.\n",
        "5. After training is done, I can test the model with some hand-crafted (or freshly fetched) test-data\n",
        "6. Establish a way of talking to the model - get a continuous conversation going\n",
        "7. Host the system on a site [not here]\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohomJB6Ru_HI"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "import torch\n",
        "import transformers\n",
        "from google.colab import drive\n",
        "import os\n",
        "import datetime\n",
        "import enchant\n",
        "import random\n",
        "\n",
        "import math\n",
        "import logging\n",
        "from pprint import pformat\n",
        "from itertools import chain\n",
        "from pytorch_pretrained_bert import cached_path\n",
        "\n",
        "from transformers import OpenAIGPTTokenizerFast\n",
        "from transformers import OpenAIGPTDoubleHeadsModel\n",
        "from transformers import GPT2TokenizerFast\n",
        "from transformers import GPT2DoubleHeadsModel\n",
        "from transformers import OpenAIGPTLMHeadModel\n",
        "from transformers import GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "from tokenizers import AddedToken\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl1vqukrqN0L",
        "outputId": "d048ddfe-8f08-4cb0-c521-0e17c38d1130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Mounting Google Drive to this colab instance. Now I should be able to my intended JSON file from drive\n",
        "'''\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMScdBXVq3HM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Here I will initialize and keep the constants\n",
        "'''\n",
        "\n",
        "driveRootPath = '/content/gdrive/My Drive/Projects/Conversational AI'\n",
        "jsonFilePath = '/prangon.json'\n",
        "modelSavePath = '/model'\n",
        "punctuationMarks = ['.', '?' ,',', '!', ';', '/', '*', '$', '&', '(', ')' ,'[', ']', '%','\"',\"'\",':','-','=','+', '_', '>', '<']\n",
        "permittedEmoji = [':3']\n",
        "specialTokens = ['<speaker1>','<speaker2>','<bos>','<eos>','<pad>']\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
        "\n",
        "MODEL_INPUTS = ['input_ids', 'token_type_ids', 'lm_labels', 'mc_token_ids', 'mc_labels']\n",
        "PADDED_INPUTS = [input_type for input_type in MODEL_INPUTS if 'mc' not in input_type]\n",
        "# print(PADDED_INPUTS)\n",
        "\n",
        "model_type = 'openai-gpt'\n",
        "pairOfExchanges = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atHMKp8kqoxg"
      },
      "outputs": [],
      "source": [
        "rawDataJson =  open(os.path.normpath(driveRootPath+jsonFilePath))\n",
        "messageFile = json.load(rawDataJson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBImizTU4Bh9"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed9zA0_isU-1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility Functions\n",
        "\"\"\"\n",
        "\n",
        "def readLists(dictName, dictKey):\n",
        "    print(dictKey,\":\")\n",
        "    for item in dictName[dictKey]:\n",
        "        print(item)\n",
        "    print(\"\\n\")\n",
        "\n",
        "def groupMessagesByDate(messageList):\n",
        "    returnDict = {}\n",
        "    for message in messageList:\n",
        "        if message['type'] == \"Generic\" and 'content' in message:\n",
        "            timeStamp = datetime.datetime.fromtimestamp(message['timestamp_ms']/1000.0).strftime('%d/%m/%Y')\n",
        "            #print(message)\n",
        "            if timeStamp in returnDict:\n",
        "                returnDict[timeStamp].append({'content' : message[\"content\"], 'sender_name':message['sender_name']})\n",
        "            else:\n",
        "                returnDict[timeStamp] = [{'content' : message[\"content\"], 'sender_name':message['sender_name']}]\n",
        "\n",
        "    return returnDict\n",
        "\n",
        "def groupMessagesBySentenceNum(messageList):\n",
        "    returnDict = []\n",
        "    currentList = []\n",
        "    for message in messageList:\n",
        "      if len(currentList) == 2*pairOfExchanges:\n",
        "        returnDict.append(currentList)\n",
        "        currentList = []\n",
        "      if message['type'] == \"Generic\" and 'content' in message:\n",
        "          currentList.append({'content' : message[\"content\"], 'sender_name':message['sender_name']})\n",
        "    return returnDict\n",
        "\n",
        "def filterGroupMessages(groupedMessage, otherName):\n",
        "    '''this function will only keep the message exchanges that are fit for training'''\n",
        "\n",
        "    '''First off, we will convert each conversation so that it begins with my texts'''\n",
        "\n",
        "    for conversationIdx, messages in enumerate(groupedMessage):\n",
        "        startIdx = -1\n",
        "        for idx, message in enumerate(messages):\n",
        "            #print(message)\n",
        "            if otherName not in message['sender_name']: \n",
        "                startIdx = idx\n",
        "                break\n",
        " \n",
        "        groupedMessage[conversationIdx] = messages[startIdx:]\n",
        "\n",
        "    '''Second, we are gonna concatenate consecutive messages from one sender and coalesce them into one message'''\n",
        "    for conversationIdx, messages in enumerate(groupedMessage):\n",
        "        if len(messages)>0:\n",
        "            newMessages = []\n",
        "            currentIdx = 1\n",
        "            prevSender = messages[0][\"sender_name\"]\n",
        "            prevContent = messages[0][\"content\"]\n",
        "\n",
        "            while currentIdx < len(messages):\n",
        "                if messages[currentIdx][\"sender_name\"] == prevSender:\n",
        "                    prevContent += (\" \"+ messages[currentIdx][\"content\"])\n",
        "                else:\n",
        "                    newMessages.append({'sender_name':prevSender,'content': prevContent})\n",
        "                    prevContent = messages[currentIdx][\"content\"]\n",
        "                    prevSender = messages[currentIdx][\"sender_name\"]\n",
        "                currentIdx += 1\n",
        "            newMessages.append({'sender_name':prevSender,'content': prevContent})\n",
        "            groupedMessage[conversationIdx] = newMessages\n",
        "\n",
        "    '''Third, we will convert the conversations so that they end with the other person's sentence. In this way\n",
        "        we are ensuring that there will be even number of exchanges in each conversation. Furthermore, in this way\n",
        "        every exchange will begin with my text and end with the robot's reply\n",
        "    '''\n",
        "    \n",
        "    for conversationIdx, messages in enumerate(groupedMessage):\n",
        "        if len(messages) > 0 and otherName not in messages[len(messages)-1]['sender_name']:\n",
        "            groupedMessage[conversationIdx] = messages[:-1]\n",
        "\n",
        "    '''Fourth, we will remove all the conversations that include fewer than one exchange from each party.\n",
        "    Moreover, we will convert the dictionary to a list of conversations (which themselves will be lists of strings)'''\n",
        "    conversationList = [messages for messages in groupedMessage if len(messages)>1]\n",
        "    conversationList = [ [[message['content'] for message in messages ]] for messages in conversationList ]\n",
        "\n",
        "\n",
        "    ''' Now we will breed more conversations from those we have :3 \n",
        "        Conversations that have more than 2 exchanges have been broken down to make more conversations.\n",
        "        Conversations that have more than 2 exchanges have been split to make conversations with 4, 6,... exchanges.\n",
        "    '''\n",
        "    # conversationCount = len(conversationList)\n",
        "    # for idx in range(conversationCount):\n",
        "    #     conversation = conversationList[idx][0]\n",
        "    #     conversationLength = len(conversation)\n",
        "\n",
        "    #     if conversationLength > 2:\n",
        "    #         endIdx = 2\n",
        "    #         while endIdx < conversationLength:\n",
        "    #             conversationList.append([conversation[:endIdx]])\n",
        "    #             endIdx += 2\n",
        "\n",
        "    # print(\"conversation count: \",len(conversationList))\n",
        "    # messagesCount2(conversationList, \"Naimul\")\n",
        "    \n",
        "    \n",
        "    # for idx in range(5):\n",
        "    #     print(conversationList[idx],\"\\n\\n\",conversationList[idx+212])\n",
        "\n",
        "    return conversationList\n",
        "def removeNewLines(conversationList):\n",
        "    \n",
        "    newConversationList = [[[sentence.replace(\"\\n\", \" \") for sentence in conversation[0]]] for conversation in conversationList]\n",
        "    return newConversationList\n",
        "\n",
        "'''places space before every punctuation mark and numbers such as ?, ., ',', !, /, *, $, &, (, ), [, ], %   '''\n",
        "def spaceBeforePunctuation_Numbers(conversationList):\n",
        "    punctuations = punctuationMarks + [str(i) for i in range(10)]\n",
        "\n",
        "    for punctuation in punctuations:\n",
        "        conversationList = [[[sentence.replace(punctuation, \" \"+punctuation+\" \") for sentence in conversation[0]]] for conversation in conversationList]\n",
        "\n",
        "    return conversationList\n",
        "\n",
        "\"\"\" Emojis appear as unicode characters that are not alphanumeric. \n",
        "    At times they appear as hexadecimal code.\n",
        "    So appear such characters or codes. We do this in order to keep \n",
        "    only Bengali words written with English letters inside nonEnglishWords list\n",
        "\"\"\"\n",
        "def removeEmoji(conversationList):\n",
        "    \n",
        "    for conversationIdx in range(len(conversationList)):\n",
        "        conversation = conversationList[conversationIdx][0]\n",
        "\n",
        "        for sentenceIdx in range(len(conversation)):\n",
        "            sentence = conversation[sentenceIdx]\n",
        "            newSentence = \"\"\n",
        "            words = sentence.split(\" \")\n",
        "            for word in words:\n",
        "                if word in punctuationMarks or word.isalnum():\n",
        "                    newSentence += (word+\" \")\n",
        "            conversationList[conversationIdx][0][sentenceIdx] = newSentence\n",
        "            \n",
        "    return conversationList\n",
        "\n",
        "def findNonEnglishWords(conversationList):\n",
        "    uniqueWords = set()\n",
        "    nonEnglishWords = []\n",
        "    wordChecker = enchant.Dict(\"en_US\")\n",
        "\n",
        "    for conversation in conversationList:\n",
        "        for sentence in conversation[0]:\n",
        "            for word in sentence.split(\" \"):\n",
        "                if len(word) > 0:\n",
        "                    uniqueWords.add(word)\n",
        "    for word in uniqueWords:\n",
        "        if wordChecker.check(word) == False:\n",
        "            nonEnglishWords.append(word)\n",
        "\n",
        "    return nonEnglishWords\n",
        "################################# USE THIS LATER IF RESPONSES BECOME TOO MEANINGLESS ################\n",
        "def removingEmptySentence(conversationList):\n",
        "\n",
        "  \"\"\"\n",
        "  This function will remove sentences that have no words or only have punctuation marks and nothing else\n",
        "  After doing this, we will again filter out conversation with fewer than two exchanges\n",
        "  \"\"\"\n",
        "\n",
        "  conversationList = [[conversation for conversation in choices] for choices in conversationList]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHo6TUFgsIpN",
        "outputId": "9178c9f3-161d-4f49-e0bc-063591686d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "participants : (length of list)  2\n",
            "\n",
            "\n",
            "messages : (length of list)  7069\n",
            "\n",
            "\n",
            "title :  Naimul Prangon\n",
            "\n",
            "\n",
            "is_still_participant :  True\n",
            "\n",
            "\n",
            "thread_type :  Regular\n",
            "\n",
            "\n",
            "thread_path :  inbox/naimulprangon_2yp3tjb4fq\n",
            "\n",
            "\n",
            "magic_words : (length of list)  0\n",
            "\n",
            "\n",
            "participants :\n",
            "{'name': 'Naimul Prangon'}\n",
            "{'name': 'Syed Zami Ul-Haque Navid'}\n",
            "\n",
            "\n",
            "magic_words :\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for key in messageFile:\n",
        "    if not isinstance(messageFile[key],list):\n",
        "        print(key,\": \",messageFile[key])\n",
        "    else:\n",
        "        print(key,\": (length of list) \",len(messageFile[key]))\n",
        "    print(\"\\n\")\n",
        "\n",
        "for key in messageFile:\n",
        "    if key != 'messages' and isinstance(messageFile[key],list):\n",
        "        readLists(messageFile, key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arO9tdUv4ICi"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFyD-IOJsfvT"
      },
      "outputs": [],
      "source": [
        "#sort messages in the increasing order of timestamp\n",
        "list.reverse(messageFile[\"messages\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLNzfOBLtlpi",
        "outputId": "97fb92c5-f655-463e-e9ea-df36d4439a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1140\n",
            "{'content': 'howard batman', 'sender_name': 'Syed Zami Ul-Haque Navid'}\n",
            "{'content': \"what's this\", 'sender_name': 'Naimul Prangon'}\n",
            "{'content': 'dongle', 'sender_name': 'Syed Zami Ul-Haque Navid'}\n",
            "{'content': 'in a hat', 'sender_name': 'Syed Zami Ul-Haque Navid'}\n",
            "{'content': 'oh', 'sender_name': 'Naimul Prangon'}\n",
            "{'content': 'tor nephew er jonne gift kinli?', 'sender_name': 'Naimul Prangon'}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "The current grouping-by-date system makes some conversation arbitrarily longer. As a result, input_ids have to contain over 1400 tokens. \n",
        "I have to make the conversations shorter so that the input_ids are able to contain 512 or fewer tokens.\n",
        "For now each group will have 4 pairs of exchanges - and hopefully 8 sentences in total\n",
        "\"\"\"\n",
        "# groupedMessages = groupMessagesByDate(messageFile[\"messages\"])\n",
        "# for date in groupedMessages:\n",
        "#     print(date,\": \",len(groupedMessages[date]))\n",
        "\n",
        "# print(groupedMessages['10/09/2021'])\n",
        "\n",
        "groupedMessages = groupMessagesBySentenceNum(messageFile[\"messages\"])\n",
        "print(len(groupedMessages))\n",
        "\n",
        "for message in groupedMessages[0]:\n",
        "  print(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXUSMtp9t1q3"
      },
      "outputs": [],
      "source": [
        "conversationList = filterGroupMessages(groupedMessages, 'Naimul')\n",
        "conversationList = removeNewLines(conversationList)\n",
        "conversationList = spaceBeforePunctuation_Numbers(conversationList)\n",
        "conversationList = removeEmoji(conversationList)\n",
        "\"\"\"\n",
        "replace empty responses with 'wut' emojis \n",
        "\"\"\"\n",
        "for conversationIdx, conversation in enumerate(conversationList):\n",
        "  for choiceIdx, choice in enumerate(conversation):\n",
        "    for messageIdx, message in enumerate(choice):\n",
        "      if len(message) < 2:\n",
        "        conversationList[conversationIdx][choiceIdx][messageIdx] = 'wut'\n",
        "\n",
        "################################# USE THIS LATER IF RESPONSES BECOME TOO MEANINGLESS ################\n",
        "# conversationList = removingEmptySentence(conversationList):\n",
        "nonEnglishWords = findNonEnglishWords(conversationList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvmHCrboj5K3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Include a distractor with every sentence\n",
        "\"\"\"\n",
        "random.shuffle(conversationList)\n",
        "for conversationIdx in range(len(conversationList)):\n",
        "  distractorConversation = conversationList[conversationIdx][0][:-1] \\\n",
        "                            + [conversationList[ (conversationIdx+1) % len(conversationList) ][0][-1]]\n",
        "  conversationList[conversationIdx].append(distractorConversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFCcTupJvQDY",
        "outputId": "42988803-a6b9-4f4f-e3d5-44807bf0ca23"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "sanity check\n",
        "\"\"\"\n",
        "for conversationIdx in range(5,7):\n",
        "    print(conversationList[conversationIdx])\n",
        "print(nonEnglishWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7De1tFEvxpU",
        "outputId": "bfb3f055-27c2-447d-cbea-e01066cdaf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1050\n",
            "900   150\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "split dataset into train and validation\n",
        "\"\"\"\n",
        "print(len(conversationList))\n",
        "# random.shuffle(conversationList)\n",
        "datasets = {'train':conversationList[:900], 'valid':conversationList[900:]}\n",
        "print(len(datasets['train']), \" \", len(datasets['valid']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIPwCqQ2HJFh",
        "outputId": "0fc67426-a2f7-4f10-cfb3-cff139f1ffce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "['nice chobi de : 3 ', 'kisher ', 'gari thelar ', 'wut']\n",
            "['nice chobi de : 3 ', 'kisher ', 'gari thelar ', 'eita to mone hoy recently shesh hoise ']\n",
            "valid\n",
            "['ikr ', 'but yeah proper way ', 'scared er pashapashi ei karoneo procrastinate kori ', 'oneke generic mail dey ']\n",
            "['ikr ', 'but yeah proper way ', 'scared er pashapashi ei karoneo procrastinate kori ', 'first e ki rakhsilen ? ']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "sanity check\n",
        "\"\"\"\n",
        "for dataset_name, dataset in datasets.items():\n",
        "  print(dataset_name)\n",
        "  for choices in dataset:\n",
        "    for conversation in choices:\n",
        "      print(conversation)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SBSd4ohKeY2"
      },
      "source": [
        "# Dataset & model preparation - Training & Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "447d1c46b5d8460fa657e0b8f58ce4a9",
            "38f8c70905f14910adb036200cbcfcf5",
            "e84a51c09e624413b2c0e3613d07fc19",
            "e8dba799ef0e45aab00269f1e5c5d648",
            "987ef4c6c28c48feaee2c701ca4342db",
            "1764117dc3a24a258e9863053612aeed",
            "2750eea162474fa9ae1fa4b517296512",
            "763f58288d974d699a042bc4df59cf6a",
            "f95524fb34b044b0a6f7345356dfd796",
            "8a3627f7c53d4b43bf0233cee49fde28",
            "0d20dd4229b0422bab97c9b98ca29c51",
            "ece94027acae4b3ea22e1c2537873884",
            "a660dfbfced143ae991bdf59d23e7148",
            "bdd075b30b6b4ede8575dabf663a88ba",
            "b963e1ded03e453f807c725bad0fce70",
            "fa06acc6f516403baab2c51da1245962",
            "8e95bd6e5b2c4ad591b964479beed724",
            "7cfcf7ed114748e796efc99e3dde2ebc",
            "9b9c95d717ae47e5ab366ebe13cf79e7",
            "91f36225fac041b78e97b761ff2ba580",
            "e0a8e0c27a3a45ff9527a15290f6c524",
            "1d852851529c4df4a0fa1f976e23f1b8",
            "33eaa263208f4b4db76ba3ea077b6924",
            "e94096c818244be8805cd0463488ce21",
            "2499b8cb91da48e5bd0688999ac101a6",
            "770cdbb4ba244665913b3bd873b66271",
            "425db998291a40cbbaf91f407bb485a1",
            "723d1cd5b2eb456d9f25d5e710f1efad",
            "54883aa4923848f7bb3ea67edb7a8a0d",
            "9fc0cd67fe8b4a07998dbe3c54869978",
            "f88e62c492114455ae9d28fb857b6526",
            "9937c872d4354516a0545498b69417de",
            "90b6c81ee1224cd8a2e94448666c30e3",
            "1c888dd81d894e688bc902f44b3b10c9",
            "3bef3620c64042f4a35a977d2cff3616",
            "27963cc6e4de4a6b9a165fef3b332539",
            "e1abb3cc11bd4b959e9e590f28b6fe56",
            "4c2fe8c0f9a043b0879c03de9a424ebf",
            "16dbdb0e850f475ba396f7b9b0182d61",
            "16f94948a71344af9528a37127db9cc9",
            "4a73ee9ce0214e5482b4597e5e95fb71",
            "0dace27754344f5daa660a7afb7f652a",
            "2dcf112086b341f0be5034690eb6e4ac",
            "e35545f680a44690823d06ed3555e18f"
          ]
        },
        "id": "0NfiAiYoJz4C",
        "outputId": "217ce3be-e925-4667-ab41-a71f94db5b9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "447d1c46b5d8460fa657e0b8f58ce4a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/816k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ece94027acae4b3ea22e1c2537873884",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/458k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33eaa263208f4b4db76ba3ea077b6924",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c888dd81d894e688bc902f44b3b10c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/656 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"\n",
        "download, load model & tokenizers\n",
        "\"\"\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer_class = OpenAIGPTTokenizerFast if model_type == \"openai-gpt\" else GPT2TokenizerFast\n",
        "tokenizer = tokenizer_class.from_pretrained(model_type)\n",
        "\n",
        "# model_class = OpenAIGPTDoubleHeadsModel if model_type == \"openai-gpt\" else GPT2DoubleHeadsModel\n",
        "# model = model_class.from_pretrained(model_type).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPgwHf9sT7yG",
        "outputId": "ca97cb58-97a3-4c45-db7d-fcef95cddcb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40478\n",
            "42824\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenizer))\n",
        "\"\"\"\n",
        "Adding new tokens to the tokenizer\n",
        "\"\"\"\n",
        "tokenizer.add_tokens([AddedToken(banglish, lstrip = True, rstrip = True) for banglish in nonEnglishWords])\n",
        "\n",
        "\"\"\"\n",
        "Adding special tokens to the tokenizer\n",
        "\"\"\"\n",
        "tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "print(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3Pv7J2MBANA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Now Tokenize the dataset\n",
        "\"\"\"\n",
        "# test_sentence = \"This is a test sentence . right ?\"\n",
        "# print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentence)))\n",
        "# print(tokenizer.encode(test_sentence))\n",
        "# print(tokenizer.decode([616, 544, 246, 2345, 5958, 239, 255, 1358, 4, 241, 257]),\" \",tokenizer.decode([616, 544, 246, 2345, 5958, 239, 255, 41066, 4, 241, 257]))\n",
        "\n",
        "def tokenize(tokenizer, token):\n",
        "\n",
        "  if isinstance(token, str):\n",
        "    return tokenizer.encode(token)\n",
        "  if isinstance(token, dict):\n",
        "    return dict((key, tokenize(tokenizer,val)) for key, val in token.items())\n",
        "  return list( tokenize(tokenizer, item) for item in token)\n",
        "\n",
        "# test_dataset = {'train':[datasets[\"train\"][0]], 'test':[datasets[\"valid\"][0]]}\n",
        "# for dataset_name, dataset in test_dataset.items():\n",
        "#   print(dataset_name)\n",
        "#   for choices in dataset:\n",
        "#     print(len(choices))\n",
        "#     for conversation in choices:\n",
        "#       print(conversation,\"\\n\")\n",
        "#     break\n",
        "# test_dataset = tokenize(tokenizer,test_dataset)\n",
        "\n",
        "# for dataset_name, dataset in test_dataset.items():\n",
        "#   print(dataset_name)\n",
        "#   for choices in dataset:\n",
        "#     print(len(choices))\n",
        "#     for conversation in choices:\n",
        "#       print(conversation,\"\\n\")\n",
        "#     break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f112UervriB"
      },
      "source": [
        "# Dataset & Model Preparation - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZjImM0YK3CM"
      },
      "outputs": [],
      "source": [
        "datasets = tokenize(tokenizer, datasets)\n",
        "# test_dataset = tokenize(tokenizer, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKM6JrP_gv66",
        "outputId": "016d412e-a220-42ce-ba81-6df520af2849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train :  900\n",
            "2\n",
            "[[40595, 697, 590, 40873, 514, 249, 1064, 6224, 42195, 507, 256, 252, 481, 2068, 697, 590, 40873, 514, 249, 825, 249, 1048, 7400, 249, 42724, 485, 5163, 258, 40904, 10019], [507, 3793, 239, 26553, 246, 41721, 20915, 3396, 40728, 41287, 42439, 795, 239, 507, 1598, 1150, 239]] \n",
            "\n",
            "[[40595, 697, 590, 40873, 514, 249, 1064, 6224, 42195, 507, 256, 252, 481, 2068, 697, 590, 40873, 514, 249, 825, 249, 1048, 7400, 249, 42724, 485, 5163, 258, 40904, 10019], [9085, 485, 704, 3165, 2057, 5687, 40613, 257]] \n",
            "\n",
            "valid :  150\n",
            "2\n",
            "[[271, 281, 41772, 40836, 41867, 42017, 246, 6494], [14433, 3317, 13810, 41867, 6319, 42488, 264, 17556, 41368, 41347, 257], [32821, 239, 7182, 249, 483, 41357, 42282, 41571], [42707, 253, 239, 41369, 2066]] \n",
            "\n",
            "[[271, 281, 41772, 40836, 41867, 42017, 246, 6494], [14433, 3317, 13810, 41867, 6319, 42488, 264, 17556, 41368, 41347, 257], [32821, 239, 7182, 249, 483, 41357, 42282, 41571], [40697, 483, 7188]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "########### SANITY CHECK #############\n",
        "\n",
        "for dataset_name, dataset in datasets.items():\n",
        "  print(dataset_name,\": \",len(dataset))\n",
        "  for choices in dataset:\n",
        "    print(len(choices))\n",
        "    for conversation in choices:\n",
        "      print(conversation,\"\\n\")\n",
        "    break\n",
        "\n",
        "# print(tokenizer.convert_tokens_to_ids(specialTokens[:-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvNaUHcit6NT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Now I would insert the bos, speaker1/2, eos tokens into the conversations to discern the beginning and end of each segment of a dialogue. \n",
        "\n",
        "2. Moreover I will convert a list of strings into a list of words (words that make up the list of strings). These list of words will be the inputs\n",
        "to the model (after tokenization)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "speaker1, speaker2, bos, eos = tokenizer.convert_tokens_to_ids(specialTokens[:-1])\n",
        "\n",
        "def joinConversationSentences(datasets):\n",
        "  for dataset_name, dataset in datasets.items():\n",
        "    conversationArray = []\n",
        "    for conversationChoices in dataset:\n",
        "      # print(len(conversationChoices),\", \",conversationChoices)\n",
        "      # break\n",
        "      choicesList = []\n",
        "      for conversation in conversationChoices:\n",
        "        wordList = [bos]\n",
        "        for sentenceIdx in range(len(conversation)):\n",
        "          sentence = conversation[sentenceIdx]\n",
        "          # print(sentence)\n",
        "          wordList += [speaker2 if sentenceIdx%2 else speaker1]\n",
        "          wordList += sentence\n",
        "        wordList += [eos]\n",
        "        choicesList.append(wordList)\n",
        "      conversationArray.append(choicesList)\n",
        "    datasets[dataset_name] = conversationArray\n",
        "  return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H7GnXLQi5U0"
      },
      "outputs": [],
      "source": [
        "datasets = joinConversationSentences(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZmrnJe_jhlE",
        "outputId": "5fc744d1-6b38-4e30-8479-a392d08ccdde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train :  900\n",
            "2\n",
            "[42819, 42822, 40595, 697, 590, 40873, 514, 249, 1064, 6224, 42195, 507, 256, 252, 481, 2068, 697, 590, 40873, 514, 249, 825, 249, 1048, 7400, 249, 42724, 485, 5163, 258, 40904, 10019, 42823, 507, 3793, 239, 26553, 246, 41721, 20915, 3396, 40728, 41287, 42439, 795, 239, 507, 1598, 1150, 239, 42820] \n",
            "\n",
            "[42819, 42822, 40595, 697, 590, 40873, 514, 249, 1064, 6224, 42195, 507, 256, 252, 481, 2068, 697, 590, 40873, 514, 249, 825, 249, 1048, 7400, 249, 42724, 485, 5163, 258, 40904, 10019, 42823, 9085, 485, 704, 3165, 2057, 5687, 40613, 257, 42820] \n",
            "\n",
            "valid :  150\n",
            "2\n",
            "[42819, 42822, 271, 281, 41772, 40836, 41867, 42017, 246, 6494, 42823, 14433, 3317, 13810, 41867, 6319, 42488, 264, 17556, 41368, 41347, 257, 42822, 32821, 239, 7182, 249, 483, 41357, 42282, 41571, 42823, 42707, 253, 239, 41369, 2066, 42820] \n",
            "\n",
            "[42819, 42822, 271, 281, 41772, 40836, 41867, 42017, 246, 6494, 42823, 14433, 3317, 13810, 41867, 6319, 42488, 264, 17556, 41368, 41347, 257, 42822, 32821, 239, 7182, 249, 483, 41357, 42282, 41571, 42823, 40697, 483, 7188, 42820] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "########## SANITY CHECK #############\n",
        "\n",
        "for dataset_name, dataset in datasets.items():\n",
        "  print(dataset_name,\": \",len(dataset))\n",
        "  for choices in dataset:\n",
        "    print(len(choices))\n",
        "    for conversation in choices:\n",
        "      print(conversation,\"\\n\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU0eT8703qQ7"
      },
      "outputs": [],
      "source": [
        "for dataset_name, dataset in datasets.items():\n",
        "  datasets[dataset_name] = {}\n",
        "  datasets[dataset_name]['input_ids'] = dataset\n",
        "  # print(dataset_name,\":\\n\",datasets[dataset_name]['input_ids'][0],\"\\n\",datasets[dataset_name]['input_ids'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCFJbX7nEACn",
        "outputId": "6675db8d-3712-4301-aa1b-2f4ef7ab9a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train : \n",
            "[42819, 42822, 40595, 697, 590, 40873, 514, 249, 1064, 6224, 42195, 507, 256, 252, 481, 2068, 697, 590, 40873, 514, 249, 825, 249, 1048, 7400, 249, 42724, 485, 5163, 258, 40904, 10019, 42823, 507, 3793, 239, 26553, 246, 41721, 20915, 3396, 40728, 41287, 42439, 795, 239, 507, 1598, 1150, 239, 42820]\n",
            "[42819, 42822, 40595, 697, 590, 40873, 514, 249, 1064, 6224, 42195, 507, 256, 252, 481, 2068, 697, 590, 40873, 514, 249, 825, 249, 1048, 7400, 249, 42724, 485, 5163, 258, 40904, 10019, 42823, 9085, 485, 704, 3165, 2057, 5687, 40613, 257, 42820]\n",
            "valid : \n",
            "[42819, 42822, 271, 281, 41772, 40836, 41867, 42017, 246, 6494, 42823, 14433, 3317, 13810, 41867, 6319, 42488, 264, 17556, 41368, 41347, 257, 42822, 32821, 239, 7182, 249, 483, 41357, 42282, 41571, 42823, 42707, 253, 239, 41369, 2066, 42820]\n",
            "[42819, 42822, 271, 281, 41772, 40836, 41867, 42017, 246, 6494, 42823, 14433, 3317, 13810, 41867, 6319, 42488, 264, 17556, 41368, 41347, 257, 42822, 32821, 239, 7182, 249, 483, 41357, 42282, 41571, 42823, 40697, 483, 7188, 42820]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "sanity check\n",
        "\"\"\"\n",
        "\n",
        "for dataset_name, dataset in datasets.items():\n",
        "  print(dataset_name,\": \")\n",
        "  for conversationChoices in dataset['input_ids']:\n",
        "    for conversation in conversationChoices:\n",
        "      print(conversation)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "XDTdMuISixFb",
        "outputId": "219d52b6-96d5-4f84-bf4b-c48244219454"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nSo far I have created inputs_ids of the dataset\\nThe dataset is now a dictionary. The following are dimenions.\\nDatasets => [key: 'train' or 'valid'] [key: 'input_ids'] [list of length 1500 for train or 113 for valid] [2 choices = reply & distractor] [variable length of list of IDs of tokenized words ]\\n\\nNext I will be creating a function that creates\\n1. segment identifiers for each conversation using inputs_ids of each conversation. Dimension will be the same as input_ids\\n2. lm_labels using input_ids of each conversation. Dimension will be the same as inputs_ids\\n3. mc_token_ids using length of each conversation. Dimensions will be [key: 'mc_token_ids'][list of length 1500 for train or 113 for valid] [2 choices = reply & distractor]\\n4. mc_labels using some of the dimensions of the input_ids. Dimensions will be [key: 'mc_token_ids'][list of length 1500 for train or 113 for valid]\\n\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "So far I have created inputs_ids of the dataset\n",
        "The dataset is now a dictionary. The following are dimenions.\n",
        "Datasets => [key: 'train' or 'valid'] [key: 'input_ids'] [list of length 1500 for train or 113 for valid] [2 choices = reply & distractor] [variable length of list of IDs of tokenized words ]\n",
        "\n",
        "Next I will be creating a function that creates\n",
        "1. segment identifiers for each conversation using inputs_ids of each conversation. Dimension will be the same as input_ids\n",
        "2. lm_labels using input_ids of each conversation. Dimension will be the same as inputs_ids\n",
        "3. mc_token_ids using length of each conversation. Dimensions will be [key: 'mc_token_ids'][list of length 1500 for train or 113 for valid] [2 choices = reply & distractor]\n",
        "4. mc_labels using some of the dimensions of the input_ids. Dimensions will be [key: 'mc_token_ids'][list of length 1500 for train or 113 for valid]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHU3ZmD-FC3Z"
      },
      "outputs": [],
      "source": [
        "def createDataset(conversationChoice):\n",
        "  \"\"\"\n",
        "  First we will create segment identifiers\n",
        "  Then, lm_labels\n",
        "  \"\"\"\n",
        "\n",
        "  segmentIds = []\n",
        "  lm_labels = []\n",
        "  replyStartIdx = -1\n",
        "  mc_token_ids = [min(511,len(conversation)-1) for conversation in conversationChoice]\n",
        "  #[len(conversation)-1 for conversation in conversationChoice]\n",
        "  mc_labels = 0 #0th index is the real reply, idx 1 contains the distractor\n",
        "\n",
        "  for conversationIdx, conversation in enumerate(conversationChoice):\n",
        "    segmentsForConversation = []\n",
        "    prevSegment, currentSegment = speaker2, speaker1 #<speaker1>\n",
        "\n",
        "    for wordIdx, word in enumerate(conversation):\n",
        "      if word == speaker2:\n",
        "        replyStartIdx = wordIdx\n",
        "      if word == prevSegment:\n",
        "        temp = currentSegment\n",
        "        currentSegment = prevSegment\n",
        "        prevSegment = temp\n",
        "      segmentsForConversation.append(currentSegment)\n",
        "    segmentIds.append(segmentsForConversation)\n",
        "\n",
        "    lm_labels.append([-100]*(len(conversation)))\n",
        "    if conversationIdx == 0: #this is the real reply, idx 1 contains the distractor\n",
        "      \"\"\"\n",
        "      We should not include <speaker2> token in the label. Hence, it should be replyStartIdx + 1 in the following line. Because replyStartIdx holds the position of the last <speaker2> token\n",
        "      \"\"\"\n",
        "      lm_labels[conversationIdx][replyStartIdx+1:] = conversation[replyStartIdx+1:]\n",
        "  \n",
        "  # for segment in segmentIds:\n",
        "  #   print(segment)\n",
        "  # for label in lm_labels:\n",
        "  #   print(label)\n",
        "  # print(mc_token_ids,\"\\n\",mc_labels)\n",
        "\n",
        "  return (segmentIds, lm_labels, mc_token_ids, mc_labels)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNrjD2jDFl13",
        "outputId": "677fda23-045f-46d0-d1aa-6c6c300bb883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train :\n",
            "valid :\n"
          ]
        }
      ],
      "source": [
        "for dataset_name, dataset in datasets.items():\n",
        "  print(dataset_name,\":\")\n",
        "  \n",
        "  for input_name in MODEL_INPUTS:\n",
        "    if input_name != 'input_ids':        \n",
        "      datasets[dataset_name][input_name] = []\n",
        "\n",
        "  for conversationChoice in dataset['input_ids']:\n",
        "    token_type_ids, lm_labels, mc_token_ids, mc_labels = createDataset(conversationChoice)\n",
        "    datasets[dataset_name]['token_type_ids'].append(token_type_ids)\n",
        "    datasets[dataset_name]['lm_labels'].append(lm_labels)\n",
        "    datasets[dataset_name]['mc_token_ids'].append(mc_token_ids)\n",
        "    datasets[dataset_name]['mc_labels'].append(mc_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdswdeBTq92w"
      },
      "outputs": [],
      "source": [
        "def sizeOfNestedList(listName, sizeStr):\n",
        "  innerItem = listName[0]\n",
        "  if isinstance(innerItem, list):\n",
        "    sizeStr += (\" \"+str(len(listName)))\n",
        "    return sizeOfNestedList(innerItem, sizeStr)\n",
        "  \n",
        "  print(sizeStr+\" \"+str(len(listName)))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y17Y1TQ1sR7i",
        "outputId": "cc26806e-eb50-45eb-ec67-c872cddf0d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train : \n",
            "input_ids\n",
            " 900 2 51\n",
            "token_type_ids\n",
            " 900 2 51\n",
            "lm_labels\n",
            " 900 2 51\n",
            "mc_token_ids\n",
            " 900 2\n",
            "mc_labels\n",
            " 900\n",
            "\n",
            "\n",
            "\n",
            "valid : \n",
            "input_ids\n",
            " 150 2 38\n",
            "token_type_ids\n",
            " 150 2 38\n",
            "lm_labels\n",
            " 150 2 38\n",
            "mc_token_ids\n",
            " 150 2\n",
            "mc_labels\n",
            " 150\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "######## SANITY CHECK ############\n",
        "for dataset_name in datasets:\n",
        "  print(dataset_name,\": \")\n",
        "  for key in datasets[dataset_name]:\n",
        "    print(key)\n",
        "    sizeOfNestedList(datasets[dataset_name][key], \"\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-iaHQTH-RV65",
        "outputId": "db622a1a-5266-4117-cede-44b9c634611f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ncreate ml_labels, mc_token_id, mc_labels\\nconsolidate creation of all the inputs except the input_ids into one function\\nOH FUCK I FORGOT TO TOKENIZE\\nI GOTTA TOKENIZE BEFORE I GO ANY FURTHER\\nBefore tokenization I will have to add the banglish tokens and the special tokens to the tokenizer and the model\\n'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "create ml_labels, mc_token_id, mc_labels\n",
        "consolidate creation of all the inputs except the input_ids into one function\n",
        "OH FUCK I FORGOT TO TOKENIZE\n",
        "I GOTTA TOKENIZE BEFORE I GO ANY FURTHER\n",
        "Before tokenization I will have to add the banglish tokens and the special tokens to the tokenizer and the model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXVz5g5Uteez"
      },
      "source": [
        "# Readying input & model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "D9VeWPlrtndH",
        "outputId": "edfca6fc-fbbc-46da-e1c1-9af64746aeed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nNow the dataset has been created as lists & dictionaries\\nNext I'll be\\n1. padding the relevant portions of the dataset\\n2. turn the dataset into tensors\\n3. One iteration for model training\\n4. Inference model\\n5. Decoding model\\n\""
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Now the dataset has been created as lists & dictionaries\n",
        "Next I'll be\n",
        "1. padding the relevant portions of the dataset\n",
        "2. turn the dataset into tensors\n",
        "3. One iteration for model training\n",
        "4. Inference model\n",
        "5. Decoding model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb8unJZ2QnvJ"
      },
      "outputs": [],
      "source": [
        "def padDataset(dataset, datasets, input_name, dataset_name):\n",
        "  max_len = -1\n",
        "  for conversationChoices in dataset:\n",
        "      for conversation in conversationChoices:\n",
        "        max_len = max(max_len, len(conversation))\n",
        "  print(max_len) \n",
        "\n",
        "\n",
        "  for choiceIdx, conversationChoices in enumerate(dataset):\n",
        "      for convIdx, conversation in enumerate(conversationChoices):\n",
        "        paddingArray = tokenizer.encode(specialTokens[-1]) if input_name != 'lm_labels' else [-100]\n",
        "        datasets[dataset_name][input_name][choiceIdx][convIdx] += ( paddingArray * (max_len - len(conversation)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w-JOBZLSrJT",
        "outputId": "e676bb51-b841-4685-87b5-97a32f7f243b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids\n",
            "428\n",
            "token_type_ids\n",
            "428\n",
            "lm_labels\n",
            "428\n",
            "input_ids\n",
            "239\n",
            "token_type_ids\n",
            "239\n",
            "lm_labels\n",
            "239\n"
          ]
        }
      ],
      "source": [
        "for dataset_name, dataset in datasets.items():\n",
        "  for input_name in PADDED_INPUTS:\n",
        "    print(input_name)\n",
        "    padDataset(dataset[input_name], datasets, input_name, dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7TkgMEjiNVD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "removing the last sentences from valid dataset, to see if we can get better at creating legit replies\n",
        "\"\"\"\n",
        "if False:\n",
        "  print(\"here\")\n",
        "  speaker2 = tokenizer.encode('<speaker2>')[0]\n",
        "  padding = tokenizer.encode(specialTokens[-1])[0]\n",
        "  # print(speaker2)\n",
        "  # lastSpeaker2Idx = -1\n",
        "\n",
        "  for choiceIdx, choicesList in enumerate(datasets['valid']['input_ids']):\n",
        "    for conversationIdx, conversation in enumerate(choicesList):\n",
        "      conversationLen = len(conversation)\n",
        "      for idx in range(conversationLen-1, 0, -1):\n",
        "        if conversation[idx] == speaker2:\n",
        "          # print(choiceIdx,\", \",conversationIdx)\n",
        "          # lastSpeaker2Idx = idx\n",
        "          datasets['valid']['input_ids'][choiceIdx][conversationIdx] = datasets['valid']['input_ids'][choiceIdx][conversationIdx][:(idx+3)] + [padding]*(conversationLen - idx-3)\n",
        "          datasets['valid']['token_type_ids'][choiceIdx][conversationIdx] = datasets['valid']['token_type_ids'][choiceIdx][conversationIdx][:(idx+3)] + [padding]*(conversationLen - idx-3)\n",
        "          break\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouja418ONdXe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creating Dataset and DataLoader\n",
        "Since I only have an iterable-type [and not map-like] dataset, I do not need a sampler. Batch-size and shuffle would suffice\n",
        "Since I do not have access to neither multiple GPU nor am I too much worried about speeding up the process, I do not need to use\n",
        "multiple workers for Data loading. Also, I do not need to think about distributed or parallel training\n",
        "\"\"\"\n",
        "\n",
        "trainDataset = TensorDataset( *[torch.tensor(input).to(device) for _,input in datasets['train'].items()] )\n",
        "validDataset = TensorDataset(*[torch.tensor(input).to(device) for _,input in datasets['valid'].items()])\n",
        "\n",
        "trainDataLoader = DataLoader(trainDataset, batch_size = 4, shuffle = True)\n",
        "validDataLoader = DataLoader(validDataset, batch_size = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbEjCCvyUA3h",
        "outputId": "b698b2e2-ec69-434b-b72b-73bb1b043b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2, 428])   cuda:0   torch.Size([4, 2, 428])   torch.Size([4, 2, 428])   torch.Size([4, 2])   torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "######### SANITY CHECK ############\n",
        "for batch in trainDataLoader:\n",
        "  input_ids, token_type_ids, lm_labels, mc_token_ids, mc_labels = batch\n",
        "  print(input_ids.shape,\" \",input_ids.device,\" \", token_type_ids.shape,\" \", lm_labels.shape, \" \", mc_token_ids.shape, \" \",mc_labels.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB-Of_AXz6Hu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "########### Model hyperparameters declaration ############\n",
        "\"\"\"\n",
        "initLR = 6.25e-5\n",
        "epochs = 25\n",
        "num_trainingSteps = epochs * len(trainDataLoader)\n",
        "train_loss = {'lm_loss':[], 'mc_loss':[], 'total_loss':[]}\n",
        "lm_loss_coeff, mc_loss_coeff = 1, 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8cUrwyCT-Ym"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=initLR)\n",
        "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0,num_training_steps=num_trainingSteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uy1bqwlK66r"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def findCELoss(lm_logits, lm_labels):\n",
        "  ######### lm_logits is a 4 dimenstional tensor - (batch_size, number_of_options, sequence_length, vocab_size) - this contains softmax probability #########\n",
        "  ######## lm_labels is a 3 dimensional tensor - (batch_size, number_of_options, sequence_length) ############\n",
        "  \"\"\"\n",
        "  1. hot encode the labels. Don't have to. The CrossEntropy function takes care of it\n",
        "  2. flatten both tensors\n",
        "  3. compute CE loss\n",
        "  \"\"\"\n",
        "  batchSize = lm_labels.shape[0] #* lm_labels.shape[1]\n",
        "  lm_labels = lm_labels[...,1:].contiguous().view(-1)\n",
        "  lm_logits = lm_logits[...,1:,:].contiguous().view(-1, lm_logits.shape[-1])\n",
        "\n",
        "  Lce = torch.nn.CrossEntropyLoss(ignore_index = -100)\n",
        "  loss = Lce(input=lm_logits, target=lm_labels) / batchSize\n",
        "\n",
        "  return loss\n",
        "\n",
        "def findPrecisionAndRecall(mc_logits, mc_labels, ):\n",
        "  ######## mc_logits is of dimensions (batch_size, num_of_options) ########\n",
        "  ######## mc_labels is of dimensions (batch_size) ###########\n",
        "\n",
        "  \"\"\"\n",
        "  1. apply argmax on mc_logits\n",
        "  2. apply condtions to count TP, TN, FP, FN\n",
        "  3. return Recall and Precision\n",
        "  \"\"\"\n",
        "  recall = 0\n",
        "  precision = 0\n",
        "  ### to avoid division by zero #####\n",
        "  TP = 1e-8\n",
        "  TN = 0\n",
        "  FN = 1e-8\n",
        "  FP = 1e-8\n",
        "\n",
        "  batchSize = mc_labels.shape[0]\n",
        "\n",
        "  mc_logits = torch.argmax(mc_logits, dim = -1)\n",
        "\n",
        "  for idx, label in enumerate(mc_labels):\n",
        "    if label == 1:\n",
        "      if mc_logits[idx] == 1:\n",
        "        TP += 1\n",
        "      else:\n",
        "        FN += 1\n",
        "    elif label == 0:\n",
        "      if mc_logits[idx] == 1:\n",
        "        FP += 1\n",
        "      else:\n",
        "        TN += 1\n",
        "\n",
        "  TP, TN = TN, TP\n",
        "  FP, FN = FN, FP\n",
        "\n",
        "  recall = TP/(TP+FN)\n",
        "  precision = TP/(TP+FP)\n",
        "\n",
        "  return precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrmpnJt-yfWN"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  logits = torch.rand(4, 2, 5, 10)\n",
        "  softmax = torch.nn.Softmax(dim=-1)\n",
        "  logits = softmax(logits)\n",
        "  labels = torch.Tensor(4,2,5).random_(0,9).to(torch.int64)\n",
        "  print(\"CE loss: \", findCELoss(logits, labels))\n",
        "\n",
        "  mc_logits = torch.rand(4,2)\n",
        "  mc_logits = softmax(mc_logits)\n",
        "  mc_labels = torch.zeros(4)\n",
        "  print(mc_labels)\n",
        "\n",
        "  print(\"precision, recall: \", findPrecisionAndRecall(mc_logits, mc_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aofXTn2T4jep"
      },
      "outputs": [],
      "source": [
        "def trainingStep(model, optimizer, dataLoader, progressbar):\n",
        "  train_loss = {'lm_loss':0, 'mc_loss':0, 'total_loss': 0}\n",
        "\n",
        "  model.train()\n",
        "  for batch in dataLoader:\n",
        "    input_ids, token_type_ids, lm_labels, mc_token_ids, mc_labels = batch\n",
        "    outputs = model(input_ids = input_ids, token_type_ids = token_type_ids, labels=lm_labels, mc_token_ids=mc_token_ids, mc_labels=mc_labels)\n",
        "    lm_loss, mc_loss = outputs.loss, outputs.mc_loss\n",
        "    loss = lm_loss * lm_loss_coeff + mc_loss * mc_loss_coeff\n",
        "    train_loss['lm_loss'] += lm_loss\n",
        "    train_loss['mc_loss'] += mc_loss\n",
        "    train_loss['total_loss'] += loss\n",
        "\n",
        "    ### maybe I can include gradient accumulation conditions here. Gradient accumulation helps to fit larger batch size in memory ######\n",
        "    ### With Gradient Accumulation will accompany gradient clipping #######\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    progressbar.update(1)\n",
        "  \n",
        "\n",
        "  for loss_type in train_loss:\n",
        "    train_loss[loss_type] /= len(dataLoader)\n",
        "\n",
        "  return train_loss\n",
        "\n",
        "def evalStep(model, optimizer, dataLoader):\n",
        "\n",
        "  loss = 0\n",
        "  CEloss = 0\n",
        "  precision = 0\n",
        "  recall = 0\n",
        "  valid_loss = {'lm_loss':0, 'mc_loss':0, 'total_loss': 0} \n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():   \n",
        "    for batch in dataLoader:\n",
        "      input_ids, token_type_ids, lm_labels, mc_token_ids, mc_labels = batch\n",
        "      outputs = model(input_ids = input_ids, token_type_ids = token_type_ids, labels=lm_labels, mc_token_ids=mc_token_ids, mc_labels=mc_labels)\n",
        "      lm_logits, mc_logits = outputs.logits, outputs.mc_logits\n",
        "      lm_loss, mc_loss = outputs.loss, outputs.mc_loss\n",
        "      loss = lm_loss * lm_loss_coeff + mc_loss * mc_loss_coeff\n",
        "      \n",
        "\n",
        "      # print(\"lm_logits.shape: \", lm_logits.shape)\n",
        "      # print(\"lm_labels.shape: \", lm_labels.shape)\n",
        "      # print(\"mc_logits.shape: \", mc_logits.shape)\n",
        "      # print(\"mc_labels.shape: \", mc_labels.shape)\n",
        "\n",
        "      # lm_logits = lm_logits[0,0,:,:]\n",
        "      # print(\"shape of the first of the batch's reply: \",lm_logits.shape)\n",
        "      # encoded_tokens = torch.argmax(lm_logits, dim=-1)\n",
        "      # encoded_tokens = encoded_tokens.view(-1) \n",
        "      # print(encoded_tokens.shape)\n",
        "\n",
        "      # decoded_text = tokenizer.decode(encoded_tokens)\n",
        "      # print(decoded_text)\n",
        "      \n",
        "\n",
        "      #### we calculate cross entropy loss to determine the difference between sentence label and the sentence output #####\n",
        "      #### we use precision and recall to determine the difference between mc label and output\n",
        "      CEloss += findCELoss(lm_logits, lm_labels)\n",
        "      currRecall, currPrecision = findPrecisionAndRecall(mc_logits, mc_labels)\n",
        "      recall += currRecall\n",
        "      precision += currPrecision\n",
        "      \n",
        "      valid_loss['lm_loss'] += lm_loss\n",
        "      valid_loss['mc_loss'] += mc_loss\n",
        "      valid_loss['total_loss'] += loss\n",
        "\n",
        "    for loss_type in train_loss:\n",
        "      valid_loss[loss_type] /= len(dataLoader)\n",
        "\n",
        "    return CEloss, precision, recall, valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b12rEnZS8oV"
      },
      "outputs": [],
      "source": [
        "def trainAndEval(model, optimizer, trainDataLoader, validDataLoader):\n",
        "  progressbar = tqdm(range(num_trainingSteps))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: \",epoch)\n",
        "    \n",
        "    train_loss = trainingStep(model, optimizer, trainDataLoader, progressbar)\n",
        "    print(\"Training Loss: \")\n",
        "    for loss_type in train_loss:\n",
        "      print(loss_type,\": \",train_loss[loss_type])\n",
        "    print(\"\\n\")\n",
        "\n",
        "    CEloss, precision, recall, valid_loss = evalStep(model, optimizer, validDataLoader)\n",
        "    print(\"Validation Loss and Metrics:\")\n",
        "    print(\"CELoss: \", CEloss, \" precision: \", precision, \" recall: \",recall)\n",
        "    for loss_type in valid_loss:\n",
        "      print(loss_type,\": \",valid_loss[loss_type])\n",
        "    print(\"\\n\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0780d12cfc3e40a5aa649b3a0ba40af4",
            "7129e6328527442e845f71f964f7392b",
            "bb752b3aa4194e40b2034bc88feea120",
            "6cc63f03c9e443888e1ebbdc1f583967",
            "66d6acaa1789402baf68abb7b141cc95",
            "997de5a5135c4cbd9a6018273d544145",
            "fa7d1ee715054fd5b4bdce8fc89bd55f",
            "366832042bbb4e01b1292aef378fba24",
            "f67d4c13fe694242bc2d01225a0ce13e",
            "a28a6c2bbfd846b3af04fa6b45800cdf",
            "65ed325bb6bd4ebcb6e40c16f909c718"
          ]
        },
        "id": "eHBF9czfV_pM",
        "outputId": "15f9a5df-07d3-445c-9cfa-f97ec09fdcd5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0780d12cfc3e40a5aa649b3a0ba40af4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5625 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "Training Loss: \n",
            "lm_loss :  tensor(6.4646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.7026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(7.1672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(65.1583, device='cuda:0')  precision:  19.99999994875  recall:  36.999999795\n",
            "lm_loss :  tensor(5.9587, device='cuda:0')\n",
            "mc_loss :  tensor(0.6931, device='cuda:0')\n",
            "total_loss :  tensor(6.6518, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Training Loss: \n",
            "lm_loss :  tensor(5.4307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.7044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(6.1350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(66.6038, device='cuda:0')  precision:  20.749999946874997  recall:  37.99999978833334\n",
            "lm_loss :  tensor(5.8189, device='cuda:0')\n",
            "mc_loss :  tensor(0.6934, device='cuda:0')\n",
            "total_loss :  tensor(6.5123, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  2\n",
            "Training Loss: \n",
            "lm_loss :  tensor(4.9953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.7090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(5.7043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(70.5184, device='cuda:0')  precision:  23.499999938749998  recall:  37.99999982666668\n",
            "lm_loss :  tensor(5.8498, device='cuda:0')\n",
            "mc_loss :  tensor(0.6859, device='cuda:0')\n",
            "total_loss :  tensor(6.5358, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  3\n",
            "Training Loss: \n",
            "lm_loss :  tensor(4.5835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.7085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(5.2920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(75.6011, device='cuda:0')  precision:  21.99999994374999  recall:  37.999999797499996\n",
            "lm_loss :  tensor(5.9596, device='cuda:0')\n",
            "mc_loss :  tensor(0.6882, device='cuda:0')\n",
            "total_loss :  tensor(6.6478, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  4\n",
            "Training Loss: \n",
            "lm_loss :  tensor(4.1574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.6883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(4.8457, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(79.5347, device='cuda:0')  precision:  18.99999995125  recall:  37.999999778333326\n",
            "lm_loss :  tensor(6.0968, device='cuda:0')\n",
            "mc_loss :  tensor(0.7404, device='cuda:0')\n",
            "total_loss :  tensor(6.8371, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  5\n",
            "Training Loss: \n",
            "lm_loss :  tensor(3.8790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.6723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(4.5513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(80.5140, device='cuda:0')  precision:  19.499999950000003  recall:  36.9999997925\n",
            "lm_loss :  tensor(6.2431, device='cuda:0')\n",
            "mc_loss :  tensor(0.6977, device='cuda:0')\n",
            "total_loss :  tensor(6.9408, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  6\n",
            "Training Loss: \n",
            "lm_loss :  tensor(3.4506, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.5867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(4.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(86.7729, device='cuda:0')  precision:  20.249999948125005  recall:  36.99999979583333\n",
            "lm_loss :  tensor(6.4404, device='cuda:0')\n",
            "mc_loss :  tensor(0.8010, device='cuda:0')\n",
            "total_loss :  tensor(7.2415, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  7\n",
            "Training Loss: \n",
            "lm_loss :  tensor(3.0137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.4270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(3.4407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(89.8442, device='cuda:0')  precision:  20.499999947500005  recall:  35.9999998075\n",
            "lm_loss :  tensor(6.6598, device='cuda:0')\n",
            "mc_loss :  tensor(0.8305, device='cuda:0')\n",
            "total_loss :  tensor(7.4902, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  8\n",
            "Training Loss: \n",
            "lm_loss :  tensor(2.6347, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.2196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(2.8543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(90.0044, device='cuda:0')  precision:  18.999999951250004  recall:  36.999999763333335\n",
            "lm_loss :  tensor(6.7609, device='cuda:0')\n",
            "mc_loss :  tensor(1.2226, device='cuda:0')\n",
            "total_loss :  tensor(7.9835, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  9\n",
            "Training Loss: \n",
            "lm_loss :  tensor(2.2800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(2.4240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(90.5400, device='cuda:0')  precision:  19.49999995  recall:  37.99999976250001\n",
            "lm_loss :  tensor(6.8997, device='cuda:0')\n",
            "mc_loss :  tensor(1.2718, device='cuda:0')\n",
            "total_loss :  tensor(8.1715, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  10\n",
            "Training Loss: \n",
            "lm_loss :  tensor(2.0356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(2.1156, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(94.9019, device='cuda:0')  precision:  18.749999953124995  recall:  36.99999977\n",
            "lm_loss :  tensor(7.0547, device='cuda:0')\n",
            "mc_loss :  tensor(1.5153, device='cuda:0')\n",
            "total_loss :  tensor(8.5700, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  11\n",
            "Training Loss: \n",
            "lm_loss :  tensor(1.7751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(1.8101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(94.6036, device='cuda:0')  precision:  18.999999951249997  recall:  37.99999975166667\n",
            "lm_loss :  tensor(7.2306, device='cuda:0')\n",
            "mc_loss :  tensor(1.6778, device='cuda:0')\n",
            "total_loss :  tensor(8.9084, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  12\n",
            "Training Loss: \n",
            "lm_loss :  tensor(1.5861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(1.6051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(96.5685, device='cuda:0')  precision:  19.749999950624996  recall:  36.99999978\n",
            "lm_loss :  tensor(7.3107, device='cuda:0')\n",
            "mc_loss :  tensor(1.5897, device='cuda:0')\n",
            "total_loss :  tensor(8.9005, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  13\n",
            "Training Loss: \n",
            "lm_loss :  tensor(1.4066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(1.4293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(96.3920, device='cuda:0')  precision:  19.99999994875  recall:  37.99999976750001\n",
            "lm_loss :  tensor(7.3566, device='cuda:0')\n",
            "mc_loss :  tensor(1.6247, device='cuda:0')\n",
            "total_loss :  tensor(8.9813, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  14\n",
            "Training Loss: \n",
            "lm_loss :  tensor(1.2802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(1.3048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(97.0689, device='cuda:0')  precision:  20.249999948125  recall:  37.999999773333336\n",
            "lm_loss :  tensor(7.4513, device='cuda:0')\n",
            "mc_loss :  tensor(1.6753, device='cuda:0')\n",
            "total_loss :  tensor(9.1266, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  15\n",
            "Training Loss: \n",
            "lm_loss :  tensor(1.1366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(1.1493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(97.4221, device='cuda:0')  precision:  19.249999950625  recall:  37.999999756666675\n",
            "lm_loss :  tensor(7.4340, device='cuda:0')\n",
            "mc_loss :  tensor(1.7525, device='cuda:0')\n",
            "total_loss :  tensor(9.1865, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  16\n",
            "Training Loss: \n",
            "lm_loss :  tensor(1.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(1.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(98.0372, device='cuda:0')  precision:  19.999999948749995  recall:  37.99999977166667\n",
            "lm_loss :  tensor(7.4651, device='cuda:0')\n",
            "mc_loss :  tensor(1.6939, device='cuda:0')\n",
            "total_loss :  tensor(9.1590, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  17\n",
            "Training Loss: \n",
            "lm_loss :  tensor(1.0069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(1.0190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(99.1770, device='cuda:0')  precision:  19.749999949375  recall:  37.99999976333334\n",
            "lm_loss :  tensor(7.5313, device='cuda:0')\n",
            "mc_loss :  tensor(1.7451, device='cuda:0')\n",
            "total_loss :  tensor(9.2764, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  18\n",
            "Training Loss: \n",
            "lm_loss :  tensor(0.9241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(0.9357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(99.5826, device='cuda:0')  precision:  20.749999946874997  recall:  37.99999977916667\n",
            "lm_loss :  tensor(7.5571, device='cuda:0')\n",
            "mc_loss :  tensor(1.7723, device='cuda:0')\n",
            "total_loss :  tensor(9.3294, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  19\n",
            "Training Loss: \n",
            "lm_loss :  tensor(0.8854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(0.8953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(99.9893, device='cuda:0')  precision:  20.749999946875  recall:  37.99999978333334\n",
            "lm_loss :  tensor(7.5969, device='cuda:0')\n",
            "mc_loss :  tensor(1.7409, device='cuda:0')\n",
            "total_loss :  tensor(9.3378, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  20\n",
            "Training Loss: \n",
            "lm_loss :  tensor(0.8427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(0.8499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(100.0346, device='cuda:0')  precision:  20.4999999475  recall:  37.99999977750001\n",
            "lm_loss :  tensor(7.6058, device='cuda:0')\n",
            "mc_loss :  tensor(1.7500, device='cuda:0')\n",
            "total_loss :  tensor(9.3558, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  21\n",
            "Training Loss: \n",
            "lm_loss :  tensor(0.8074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(0.8160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(100.2639, device='cuda:0')  precision:  19.99999994875  recall:  37.99999976333334\n",
            "lm_loss :  tensor(7.6367, device='cuda:0')\n",
            "mc_loss :  tensor(1.7533, device='cuda:0')\n",
            "total_loss :  tensor(9.3900, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  22\n",
            "Training Loss: \n",
            "lm_loss :  tensor(0.8089, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(0.8190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(100.2143, device='cuda:0')  precision:  20.249999948124998  recall:  37.99999976833334\n",
            "lm_loss :  tensor(7.6176, device='cuda:0')\n",
            "mc_loss :  tensor(1.7616, device='cuda:0')\n",
            "total_loss :  tensor(9.3793, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  23\n",
            "Training Loss: \n",
            "lm_loss :  tensor(0.7863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(0.7922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(100.2373, device='cuda:0')  precision:  20.249999948124998  recall:  37.99999976833334\n",
            "lm_loss :  tensor(7.6236, device='cuda:0')\n",
            "mc_loss :  tensor(1.7644, device='cuda:0')\n",
            "total_loss :  tensor(9.3880, device='cuda:0')\n",
            "\n",
            "\n",
            "Epoch:  24\n",
            "Training Loss: \n",
            "lm_loss :  tensor(0.7972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "mc_loss :  tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "total_loss :  tensor(0.8057, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "\n",
            "Validation Loss and Metrics:\n",
            "CELoss:  tensor(100.2569, device='cuda:0')  precision:  20.249999948124998  recall:  37.99999976833334\n",
            "lm_loss :  tensor(7.6263, device='cuda:0')\n",
            "mc_loss :  tensor(1.7651, device='cuda:0')\n",
            "total_loss :  tensor(9.3914, device='cuda:0')\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainAndEval(model, optimizer, trainDataLoader, validDataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI9Wnrk3AduJ"
      },
      "outputs": [],
      "source": [
        "torch.save(model, driveRootPath + modelSavePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GZb0ecVwd_I"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  a = ['st','ri','ng']\n",
        "  b = \" \".join(a)\n",
        "  print(b)\n",
        "\n",
        "  d = tokenizer.encode('Komge')\n",
        "  print(d)\n",
        "\n",
        "  t = \" \".join([tokenizer.decode(token) for token in d])\n",
        "  print(t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3QQfxvznQs0"
      },
      "source": [
        "# Decoding and talking to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is2G7wj0kd6d"
      },
      "outputs": [],
      "source": [
        "def createDataset2(history, currentOutput, tokenizer, withEos = True):\n",
        "  speaker1, speaker2, bos, eos = tokenizer.convert_tokens_to_ids(specialTokens[:-1])\n",
        "  \"\"\"\n",
        "  1. we convert the history so that it includes speaker1, speaker2 and bos\n",
        "  2. then we create input_ids, token_type_ids with it\n",
        "  3. return them\n",
        "  \"\"\"  \n",
        "  ### history should have several items in it. Each item is an array of tokens\n",
        "  sequence = [[speaker2]+sentence if idx%2 else [speaker1]+sentence for idx, sentence in enumerate(history) ]\n",
        "  sequence[0] = [bos] + sequence[0]\n",
        "  sequence[-1] = sequence[-1] + ([eos] if withEos else [])\n",
        "\n",
        "  input_ids = list(chain(*sequence))\n",
        "  token_type_ids = [ speaker2 if idx%2 else speaker1 for idx, sentence in enumerate(sequence) for _ in range(len(sentence))]\n",
        "\n",
        "  return input_ids, token_type_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWleXXPyC3ID"
      },
      "outputs": [],
      "source": [
        "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
        "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
        "                whose total probability mass is greater than or equal to the threshold top_p.\n",
        "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
        "                the threshold top_p.\n",
        "            threshold: a minimal threshold to keep logits\n",
        "    \"\"\"\n",
        "    # print(\"logits shape: \", logits.shape)\n",
        "\n",
        "    # if False:\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "        top_logits, top_logits_indices = torch.topk(logits, top_k)\n",
        "        # print(\"top_logits shape: \",top_logits.shape,\"\\ntop_logits_index shape: \",top_logits_indices.shape)\n",
        "        indices_to_remove = logits < top_logits[..., -1, None]\n",
        "        # print(\"indices_to_remove shape: \",indices_to_remove.shape)\n",
        "        # print(\"sample row of indices to remove: \",indices_to_remove.shape[...,0])\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        # print(\"cum sum prob shape: \",cumulative_probabilities.shape)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # print(\"indices_to_remove shape: \",sorted_indices_to_remove.shape)\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        # print(\"sample row of indices to remove: \",sorted_indices_to_remove[...,0])\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Back to unsorted indices and set them to -infinity\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    indices_to_remove = logits < threshold\n",
        "    logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8EbpclJC81E"
      },
      "outputs": [],
      "source": [
        "def sample_sequence(history, tokenizer, model):\n",
        "                    \n",
        "    special_tokens_ids = tokenizer.convert_tokens_to_ids(specialTokens)\n",
        "    current_output = []\n",
        "\n",
        "    for i in range(max_output_length):\n",
        "        input_ids, token_type_ids = createDataset2(history, current_output, tokenizer, withEos=False)\n",
        "        # print(\"input_ids: \",input_ids,\"\\ntoken_type_ids: \",token_type_ids)      \n",
        "        \n",
        "        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
        "        token_type_ids = torch.tensor(token_type_ids).to(device).unsqueeze(0)\n",
        "\n",
        "        outputs = model(input_ids = input_ids, token_type_ids= token_type_ids)\n",
        "        logits = outputs.logits\n",
        "        # print(\"logits.shape: \",logits.shape)\n",
        "\n",
        "        \n",
        "        # makes the softmax scores bigger. Their sum will exceed 1. But if we choose to use top_p, we would choose fewer options because of using temparature.\n",
        "        # If we didn't use the temparature, the probabilities would be smaller which would lead to smaller cum sum. Meaning, it will take many probabilites scores to add to a small cumulative probability\n",
        "        # Then we would end up choosing a lot of options, if not almost all\n",
        "        encoded_text = torch.argmax(logits[0], dim =-1)\n",
        "        # print(\"decoded text: \",\" \".join([tokenizer.decode(token) for token in encoded_text]) )\n",
        "        logits = logits[0, 0, :] / temperature \n",
        "        # print(\"logits.shape after dividing with temparature: \",logits.shape)\n",
        "        # if False:\n",
        "        logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        prev = torch.multinomial(probs, 1)\n",
        "        if i < min_output_length and prev.item() in special_tokens_ids:\n",
        "            while prev.item() in special_tokens_ids:\n",
        "                if probs.max().item() == 1:\n",
        "                    print(\"Warning: model generating special token with probability 1.\")\n",
        "                    break  # avoid infinitely looping over special token\n",
        "                prev = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if prev.item() in special_tokens_ids:\n",
        "            break\n",
        "        current_output.append(prev.item())\n",
        "\n",
        "    return current_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ9oiUF3DGoa"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "\t\n",
        "  if random_seed != 0:\n",
        "    random.seed(random_seed)\n",
        "    torch.random.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "  tokenizer_class, model_class = (GPT2TokenizerFast, GPT2LMHeadModel) if model_type == 'gpt2' else (OpenAIGPTTokenizerFast, OpenAIGPTLMHeadModel)\n",
        "  # tokenizer = tokenizer_class.from_pretrained(model_type)\n",
        "  \n",
        "  model = torch.load(driveRootPath+modelSavePath)\n",
        "  model.to(device)\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "  \n",
        "  # print(\"tokenizer size: \", len(tokenizer))\n",
        "  # print(tokenizer.convert_tokens_to_ids(specialTokens[:-1]))\n",
        "  # print(tokenizer.encode(\"wut do\"))\n",
        "\n",
        "\n",
        "  history = []\n",
        "  while True:\n",
        "      raw_text = input(\">>> \")\n",
        "      while not raw_text:\n",
        "          print('Prompt should not be empty!')\n",
        "          raw_text = input(\">>> \")\n",
        "      history.append(tokenizer.encode(raw_text))\n",
        "      with torch.no_grad():\n",
        "          out_ids = sample_sequence(history, tokenizer, model)\n",
        "      history.append(out_ids)\n",
        "      history = history[-(2*pairOfExchanges+1):]\n",
        "      out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "      print(out_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr1NwYsmgqAQ"
      },
      "outputs": [],
      "source": [
        "random_seed = 0\n",
        "temperature = 0.7\n",
        "top_k = 0\n",
        "top_p = 0.9\n",
        "max_output_length = 20\n",
        "min_output_length = 1\n",
        "model_type = 'openai-gpt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31uF1uEOsNvr",
        "outputId": "d1983e50-a061-4e79-a7de-79dcad31a35d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> hello\n",
            "?, boltesos? kortesi,? hoiche?'eddinfalateto AbarratargularehEtaiSGDmushkileddin\n",
            ">>> wut do\n",
            "?? shara,???! chash,?'chash???,,??\n"
          ]
        }
      ],
      "source": [
        "run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QI5D7BqJbfZL",
        "Fhjn-G2LjXjd",
        "HPukiBYwjwmy",
        "5QwJ8xwWb4yL",
        "dGW2XzD5hZ1h",
        "SBImizTU4Bh9",
        "arO9tdUv4ICi",
        "I3QQfxvznQs0"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0780d12cfc3e40a5aa649b3a0ba40af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7129e6328527442e845f71f964f7392b",
              "IPY_MODEL_bb752b3aa4194e40b2034bc88feea120",
              "IPY_MODEL_6cc63f03c9e443888e1ebbdc1f583967"
            ],
            "layout": "IPY_MODEL_66d6acaa1789402baf68abb7b141cc95"
          }
        },
        "0bcfa8f47efb4278bb344355b6e99ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d20dd4229b0422bab97c9b98ca29c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dace27754344f5daa660a7afb7f652a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0daf4d93e63b42738f796d28af978802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbf7299be91045f297cdb325e9cc2b8a",
            "max": 656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b31ba3591eb429ab48d39f399f85e1b",
            "value": 656
          }
        },
        "0f0d4af742c94050b2ca184dab934982": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd2b2d06d6b045f48a49c449e8ae46fa",
            "placeholder": "​",
            "style": "IPY_MODEL_181d43730b724d4996ed2166c2c2f4b2",
            "value": " 656/656 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "16dbdb0e850f475ba396f7b9b0182d61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f94948a71344af9528a37127db9cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1764117dc3a24a258e9863053612aeed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181d43730b724d4996ed2166c2c2f4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c888dd81d894e688bc902f44b3b10c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bef3620c64042f4a35a977d2cff3616",
              "IPY_MODEL_27963cc6e4de4a6b9a165fef3b332539",
              "IPY_MODEL_e1abb3cc11bd4b959e9e590f28b6fe56"
            ],
            "layout": "IPY_MODEL_4c2fe8c0f9a043b0879c03de9a424ebf"
          }
        },
        "1d852851529c4df4a0fa1f976e23f1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "246b458aa4b24630ab57c34a64d5c637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f214cff051f44a078e842e06810aac4c",
              "IPY_MODEL_0daf4d93e63b42738f796d28af978802",
              "IPY_MODEL_0f0d4af742c94050b2ca184dab934982"
            ],
            "layout": "IPY_MODEL_26ebe65f2ee34317aa7d70e9658e4972"
          }
        },
        "2499b8cb91da48e5bd0688999ac101a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fc0cd67fe8b4a07998dbe3c54869978",
            "max": 1272610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f88e62c492114455ae9d28fb857b6526",
            "value": 1272610
          }
        },
        "26ebe65f2ee34317aa7d70e9658e4972": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2750eea162474fa9ae1fa4b517296512": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27963cc6e4de4a6b9a165fef3b332539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a73ee9ce0214e5482b4597e5e95fb71",
            "max": 656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dace27754344f5daa660a7afb7f652a",
            "value": 656
          }
        },
        "2dcf112086b341f0be5034690eb6e4ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33eaa263208f4b4db76ba3ea077b6924": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e94096c818244be8805cd0463488ce21",
              "IPY_MODEL_2499b8cb91da48e5bd0688999ac101a6",
              "IPY_MODEL_770cdbb4ba244665913b3bd873b66271"
            ],
            "layout": "IPY_MODEL_425db998291a40cbbaf91f407bb485a1"
          }
        },
        "366832042bbb4e01b1292aef378fba24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f8c70905f14910adb036200cbcfcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1764117dc3a24a258e9863053612aeed",
            "placeholder": "​",
            "style": "IPY_MODEL_2750eea162474fa9ae1fa4b517296512",
            "value": "Downloading: 100%"
          }
        },
        "3b4ccab2b0204d7e99c4d76744db2b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c2495a39c0a48ec89691a659df0aa28",
            "placeholder": "​",
            "style": "IPY_MODEL_0bcfa8f47efb4278bb344355b6e99ab9",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "3bef3620c64042f4a35a977d2cff3616": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16dbdb0e850f475ba396f7b9b0182d61",
            "placeholder": "​",
            "style": "IPY_MODEL_16f94948a71344af9528a37127db9cc9",
            "value": "Downloading: 100%"
          }
        },
        "425db998291a40cbbaf91f407bb485a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "447d1c46b5d8460fa657e0b8f58ce4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38f8c70905f14910adb036200cbcfcf5",
              "IPY_MODEL_e84a51c09e624413b2c0e3613d07fc19",
              "IPY_MODEL_e8dba799ef0e45aab00269f1e5c5d648"
            ],
            "layout": "IPY_MODEL_987ef4c6c28c48feaee2c701ca4342db"
          }
        },
        "4a73ee9ce0214e5482b4597e5e95fb71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c2fe8c0f9a043b0879c03de9a424ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54883aa4923848f7bb3ea67edb7a8a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "549a4f7847d94e54ac89697ec153e60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c32d1cc2134baf9ceda62ee234a55b",
            "max": 478750579,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb8aa361c379455fbedce1765ff40d57",
            "value": 478750579
          }
        },
        "65ed325bb6bd4ebcb6e40c16f909c718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66d6acaa1789402baf68abb7b141cc95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2495a39c0a48ec89691a659df0aa28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cc63f03c9e443888e1ebbdc1f583967": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a28a6c2bbfd846b3af04fa6b45800cdf",
            "placeholder": "​",
            "style": "IPY_MODEL_65ed325bb6bd4ebcb6e40c16f909c718",
            "value": " 5625/5625 [1:31:31&lt;00:00,  1.06it/s]"
          }
        },
        "7129e6328527442e845f71f964f7392b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_997de5a5135c4cbd9a6018273d544145",
            "placeholder": "​",
            "style": "IPY_MODEL_fa7d1ee715054fd5b4bdce8fc89bd55f",
            "value": "100%"
          }
        },
        "723d1cd5b2eb456d9f25d5e710f1efad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763f58288d974d699a042bc4df59cf6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "770cdbb4ba244665913b3bd873b66271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9937c872d4354516a0545498b69417de",
            "placeholder": "​",
            "style": "IPY_MODEL_90b6c81ee1224cd8a2e94448666c30e3",
            "value": " 1.27M/1.27M [00:00&lt;00:00, 5.39MB/s]"
          }
        },
        "7b31ba3591eb429ab48d39f399f85e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cfcf7ed114748e796efc99e3dde2ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "884f5293cc4741b3a20e79cb467f71db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a3627f7c53d4b43bf0233cee49fde28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e95bd6e5b2c4ad591b964479beed724": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3cb368fa8242f2a10d7055a8cc7cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90b6c81ee1224cd8a2e94448666c30e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91f36225fac041b78e97b761ff2ba580": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "987ef4c6c28c48feaee2c701ca4342db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9937c872d4354516a0545498b69417de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "997de5a5135c4cbd9a6018273d544145": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9c95d717ae47e5ab366ebe13cf79e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fc0cd67fe8b4a07998dbe3c54869978": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fcefbf2f4ef42af82190d139556f493": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b4ccab2b0204d7e99c4d76744db2b54",
              "IPY_MODEL_549a4f7847d94e54ac89697ec153e60f",
              "IPY_MODEL_fd7ccbb1380a4769bb4c5ba48588382d"
            ],
            "layout": "IPY_MODEL_d8c0d5b5af3645b885487c1dd16846f4"
          }
        },
        "a28a6c2bbfd846b3af04fa6b45800cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a660dfbfced143ae991bdf59d23e7148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e95bd6e5b2c4ad591b964479beed724",
            "placeholder": "​",
            "style": "IPY_MODEL_7cfcf7ed114748e796efc99e3dde2ebc",
            "value": "Downloading: 100%"
          }
        },
        "b963e1ded03e453f807c725bad0fce70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0a8e0c27a3a45ff9527a15290f6c524",
            "placeholder": "​",
            "style": "IPY_MODEL_1d852851529c4df4a0fa1f976e23f1b8",
            "value": " 458k/458k [00:00&lt;00:00, 1.96MB/s]"
          }
        },
        "bb752b3aa4194e40b2034bc88feea120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_366832042bbb4e01b1292aef378fba24",
            "max": 5625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f67d4c13fe694242bc2d01225a0ce13e",
            "value": 5625
          }
        },
        "bbf7299be91045f297cdb325e9cc2b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd075b30b6b4ede8575dabf663a88ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b9c95d717ae47e5ab366ebe13cf79e7",
            "max": 458495,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91f36225fac041b78e97b761ff2ba580",
            "value": 458495
          }
        },
        "c86959439f954c21a04b24b1b16e1652": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8aa361c379455fbedce1765ff40d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd2b2d06d6b045f48a49c449e8ae46fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c0d5b5af3645b885487c1dd16846f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0a8e0c27a3a45ff9527a15290f6c524": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1abb3cc11bd4b959e9e590f28b6fe56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dcf112086b341f0be5034690eb6e4ac",
            "placeholder": "​",
            "style": "IPY_MODEL_e35545f680a44690823d06ed3555e18f",
            "value": " 656/656 [00:00&lt;00:00, 17.7kB/s]"
          }
        },
        "e1c32d1cc2134baf9ceda62ee234a55b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35545f680a44690823d06ed3555e18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e50072d0d16f49cdb479145e026c742a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e84a51c09e624413b2c0e3613d07fc19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_763f58288d974d699a042bc4df59cf6a",
            "max": 815973,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f95524fb34b044b0a6f7345356dfd796",
            "value": 815973
          }
        },
        "e8dba799ef0e45aab00269f1e5c5d648": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a3627f7c53d4b43bf0233cee49fde28",
            "placeholder": "​",
            "style": "IPY_MODEL_0d20dd4229b0422bab97c9b98ca29c51",
            "value": " 816k/816k [00:00&lt;00:00, 1.03MB/s]"
          }
        },
        "e94096c818244be8805cd0463488ce21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_723d1cd5b2eb456d9f25d5e710f1efad",
            "placeholder": "​",
            "style": "IPY_MODEL_54883aa4923848f7bb3ea67edb7a8a0d",
            "value": "Downloading: 100%"
          }
        },
        "ece94027acae4b3ea22e1c2537873884": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a660dfbfced143ae991bdf59d23e7148",
              "IPY_MODEL_bdd075b30b6b4ede8575dabf663a88ba",
              "IPY_MODEL_b963e1ded03e453f807c725bad0fce70"
            ],
            "layout": "IPY_MODEL_fa06acc6f516403baab2c51da1245962"
          }
        },
        "f214cff051f44a078e842e06810aac4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e50072d0d16f49cdb479145e026c742a",
            "placeholder": "​",
            "style": "IPY_MODEL_884f5293cc4741b3a20e79cb467f71db",
            "value": "Downloading config.json: 100%"
          }
        },
        "f67d4c13fe694242bc2d01225a0ce13e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f88e62c492114455ae9d28fb857b6526": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f95524fb34b044b0a6f7345356dfd796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa06acc6f516403baab2c51da1245962": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7d1ee715054fd5b4bdce8fc89bd55f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd7ccbb1380a4769bb4c5ba48588382d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86959439f954c21a04b24b1b16e1652",
            "placeholder": "​",
            "style": "IPY_MODEL_8f3cb368fa8242f2a10d7055a8cc7cd7",
            "value": " 457M/457M [00:08&lt;00:00, 58.5MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
